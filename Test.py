from matplotlib import pyplot as plt
from Model import MyRNN
from Dataloading import get_patient_data
import numpy as np
import pandas as pd
import torch
from train_and_evaluate import rnn_training

device = 'cuda' if torch.cuda.is_available() else 'cpu'

X, y = get_patient_data(use_interpolation=True)

# df = pd.DataFrame({'accuracy': [0.31527093596059114, 0.5788177339901478, 0.5960591133004927, 0.7142857142857143, 0.8029556650246306, 0.8054187192118226, 0.8029556650246306, 0.8054187192118226, 0.8497536945812808, 0.8645320197044335, 0.8620689655172413, 0.8620689655172413, 0.8645320197044335, 0.8620689655172413, 0.8620689655172413, 0.8620689655172413, 0.8694581280788177, 0.8571428571428571, 0.8719211822660099, 0.8891625615763546, 0.9064039408866995, 0.916256157635468, 0.9187192118226601, 0.9187192118226601, 0.9187192118226601, 0.9187192118226601, 0.916256157635468, 0.9187192118226601, 0.9211822660098522, 0.9137931034482759, 0.916256157635468, 0.916256157635468, 0.9211822660098522, 0.9187192118226601, 0.9211822660098522, 0.9187192118226601, 0.9187192118226601, 0.9187192118226601, 0.916256157635468, 0.9187192118226601, 0.9187192118226601, 0.9211822660098522, 0.916256157635468, 0.9211822660098522, 0.9211822660098522, 0.9236453201970444, 0.9113300492610837, 0.9211822660098522, 0.9211822660098522, 0.9211822660098522, 0.9211822660098522, 0.9211822660098522, 0.9211822660098522, 0.9236453201970444, 0.9261083743842364, 0.9236453201970444, 0.9236453201970444, 0.9359605911330049, 0.9359605911330049, 0.9236453201970444, 0.9261083743842364, 0.9261083743842364, 0.9310344827586207, 0.9310344827586207, 0.9334975369458128, 0.9310344827586207, 0.9285714285714286, 0.9310344827586207, 0.9310344827586207, 0.9310344827586207, 0.9359605911330049, 0.9359605911330049, 0.9334975369458128, 0.9359605911330049, 0.9310344827586207, 0.9334975369458128, 0.9359605911330049, 0.9359605911330049, 0.9359605911330049, 0.9334975369458128, 0.9384236453201971, 0.9359605911330049, 0.9334975369458128, 0.9334975369458128, 0.9359605911330049, 0.9384236453201971, 0.9408866995073891, 0.9384236453201971, 0.9310344827586207, 0.9384236453201971, 0.9408866995073891, 0.9310344827586207, 0.9334975369458128, 0.9334975369458128, 0.9408866995073891, 0.9334975369458128, 0.9408866995073891, 0.9310344827586207, 0.9359605911330049, 0.9433497536945813], 'bal_accuracy': [0.25, 0.43342620309640023, 0.45892336218730934, 0.5557363960494204, 0.6266881005948749, 0.6283216233221476, 0.6263241687004271, 0.6276824187766931, 0.6593931131787675, 0.6706056474984747, 0.6686525224984747, 0.6689277951494814, 0.6712005224222086, 0.6695226700732154, 0.6689277951494814, 0.6698422723459426, 0.6766604541641245, 0.6660246815893838, 0.6789331814368518, 0.6948422723459426, 0.7116658404514948, 0.7210763518151312, 0.7233490790878584, 0.7233490790878584, 0.7233490790878584, 0.7233490790878584, 0.7207567495424039, 0.7233490790878584, 0.7253022040878584, 0.7202686470408786, 0.7216712267388652, 0.7216712267388652, 0.7255774767388652, 0.7242192266625992, 0.7258970790115924, 0.7233490790878584, 0.7236243517388652, 0.7239439540115924, 0.7213516244661379, 0.7236243517388652, 0.7236243517388652, 0.7255774767388652, 0.7219908290115924, 0.7258970790115924, 0.7250269314368517, 0.727299658709579, 0.7188660673428919, 0.7256218063605857, 0.7255774767388652, 0.7258970790115924, 0.7258970790115924, 0.7255774767388652, 0.7255774767388652, 0.7387353714757073, 0.7518932662125494, 0.7499401412125494, 0.7611449109493915, 0.8045248451599178, 0.8045248451599178, 0.7611449109493915, 0.7972628904619312, 0.7975381631129379, 0.8006185951599178, 0.8006185951599178, 0.8243420550881474, 0.8118233648967599, 0.7986654701599178, 0.8115037626240326, 0.8118233648967599, 0.8329544952795351, 0.8157296148967599, 0.8151790695947464, 0.8137764898967599, 0.8266147823608747, 0.8118233648967599, 0.8358664270977169, 0.826934384633602, 0.826934384633602, 0.8374999498249895, 0.8140960921694871, 0.8397726770977169, 0.826934384633602, 0.8246616573608747, 0.8246616573608747, 0.8380948247487235, 0.8727477817627887, 0.864135341571401, 0.8509774468345589, 0.8227085323608747, 0.8727477817627887, 0.8862252787723579, 0.8227085323608747, 0.8358664270977169, 0.8246616573608747, 0.8750205090355159, 0.8358664270977169, 0.864135341571401, 0.8227085323608747, 0.8381391543704442, 0.8772932363082431], 'macro': [0.1198501872659176, 0.4167889815522127, 0.44435292403272697, 0.5459566145457122, 0.6107462686567164, 0.6116767523017523, 0.6090645081562036, 0.6103110599078341, 0.6432845842661643, 0.654445922014424, 0.6527884435540144, 0.653263709337475, 0.6549889023405973, 0.6532214188993132, 0.6529079373285978, 0.6535702727855645, 0.6601904601652203, 0.6505863120342267, 0.6625052749800352, 0.6775278293135435, 0.6928831849264862, 0.7013960536541365, 0.7034737769428113, 0.7034737769428113, 0.7034737769428113, 0.7034915237027058, 0.7012567604224297, 0.7035239738362334, 0.7054117614389352, 0.6999270346457768, 0.7017125909084156, 0.7016873870082606, 0.7054452498454373, 0.703821689060548, 0.7056395537476092, 0.7034915237027058, 0.7036039246378586, 0.7037932231968129, 0.7015319109851467, 0.7036073044790354, 0.7036073044790354, 0.7054452498454373, 0.7018785663279679, 0.7056395537476092, 0.7052969432371714, 0.7073859051817448, 0.6982971911486032, 0.7056119376078076, 0.7054636046617719, 0.7056395537476092, 0.7056395537476092, 0.7054636046617719, 0.7054636046617719, 0.7313733757735632, 0.7549097444721349, 0.7517603161021378, 0.7687123305403459, 0.8310373922327465, 0.8310373922327465, 0.7687371050490572, 0.8134469168761577, 0.8166304906415666, 0.8202345676670905, 0.8202345676670905, 0.8467136853456625, 0.8297532045523499, 0.8182686487425157, 0.8295074352565052, 0.8297509416326677, 0.8479988059291476, 0.8369263057365404, 0.8404242241472786, 0.8349590259200053, 0.8450604503113629, 0.8331047678304891, 0.8504591823648149, 0.845213537096951, 0.845213537096951, 0.8521494939305789, 0.835198826893238, 0.8617641429737107, 0.8451909734713385, 0.8397486138557753, 0.8397486138557753, 0.8560426272766207, 0.8781219020367701, 0.8794889560354029, 0.8680833357935758, 0.8378943557662593, 0.8781219020367701, 0.8882414828435405, 0.8378943557662593, 0.8540013704626651, 0.8468770568049335, 0.8801449378835968, 0.8504288917421939, 0.8794889560354029, 0.8378943557662593, 0.8561290300371331, 0.8860839833327998], 'weighted': [0.15114112285751186, 0.5438943064531384, 0.5645340279097468, 0.6967591365123236, 0.7832563782074846, 0.7852802153910529, 0.783098919080817, 0.7846248760905921, 0.826487072478088, 0.8429928503619676, 0.8405685280610706, 0.8406632062430515, 0.8430642859130794, 0.8408165171141647, 0.8405415142506781, 0.8411558310161753, 0.8487412418128606, 0.8359030997759335, 0.8512499081146303, 0.8686698593454397, 0.8861629156950428, 0.8960075076108637, 0.8984224431974802, 0.8984224431974802, 0.8984224431974802, 0.8986061620672215, 0.8957128125873544, 0.898807100673912, 0.9008664039413606, 0.8937359868178117, 0.8963414256981621, 0.8961496251985818, 0.9009001426349453, 0.8987881938400762, 0.9012633229667454, 0.8986061620672215, 0.8982425232985312, 0.8987620691032333, 0.8958349654153733, 0.8984094631911624, 0.8984094631911624, 0.9009001426349453, 0.8965064173531666, 0.9012633229667454, 0.9010630828990356, 0.9034940536822013, 0.8908208961066609, 0.901236989693236, 0.9010846283147786, 0.9012633229667454, 0.9012633229667454, 0.9010846283147786, 0.9010846283147786, 0.9067503901108086, 0.9119786386182918, 0.910494064076445, 0.9134043253888062, 0.9295882404606625, 0.9295882404606625, 0.9135982457488699, 0.921013996398948, 0.9204138806414014, 0.9253378009319961, 0.9253378009319961, 0.9300023582046251, 0.9273902802910389, 0.9231936119070313, 0.9269548391247353, 0.9268827982677214, 0.9289798202944219, 0.9314755665015477, 0.9314414750412238, 0.9291642908248902, 0.9328196312295983, 0.92665303967117, 0.9314521588709317, 0.9328062898015473, 0.9328062898015473, 0.9339649822783381, 0.9292578189083895, 0.9358138033495842, 0.9329489077500288, 0.9307398493378335, 0.9307398493378335, 0.9336120931939006, 0.938264413987259, 0.9397459503404574, 0.9367242600947989, 0.9282285981841131, 0.938264413987259, 0.9411407683182367, 0.9282285981841131, 0.9313909821629199, 0.9300002149660037, 0.9407779228379778, 0.9315863492575308, 0.9397459503404574, 0.9282285981841131, 0.9336968201253978, 0.9429450328486082]})
#
# plt.plot(df["bal_accuracy"], label="bac")
# plt.plot(df["accuracy"], label="acc")
# plt.plot(df["macro"], label="mac")
# plt.plot(df["weighted"], label="wgt")
# plt.legend()
# plt.show()

window_size = 10
hop_size = window_size//2
output = torch.tensor([]) # shape [num_files*num_sliding_widows, window_size, features]
labels = []
for file in X["file_name"].unique():
    features_df = X[X['file_name'] == file]
    labels_df = y[y['file_name'] == file].values
    last_frame = np.max(X[X['file_name'] == file]["frame_id"])
    for pivot in range(0, last_frame - window_size, hop_size):
        labels.append(labels_df[pivot+window_size-1][0])
        window = torch.tensor(features_df[features_df["frame_id"].between(pivot, pivot + window_size, inclusive="left")].drop(["frame_id", "file_name"], axis=1).values)
        output = torch.cat((output, torch.unsqueeze(window, dim=0)), dim=0)

labels = torch.tensor(labels) - 1
output = output.to(torch.float32)

rnn_training(output, labels, "First Test", device)
